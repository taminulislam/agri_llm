{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Agricultural PDF Text Extraction Pipeline\n",
    "\n",
    "This notebook extracts text from agricultural PDFs and creates a unified dataset for LLM fine-tuning.\n",
    "\n",
    "## Features:\n",
    "- **Smart Detection**: Automatically detects digital vs scanned PDFs\n",
    "- **Fast Extraction**: Uses PyMuPDF for digital PDFs (instant)\n",
    "- **High-Quality OCR**: Uses Chandra for scanned PDFs\n",
    "- **JSONL Output**: Best format for LLM training\n",
    "- **Rich Metadata**: Includes source, page numbers, timestamps\n",
    "\n",
    "## Output Format:\n",
    "```json\n",
    "{\"id\": \"chapter02-p1\", \"doc\": \"chapter02\", \"page\": 1, \"text\": \"...\", \"source\": \"chapter02.pdf\", \"method\": \"digital\", \"timestamp\": \"...\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pymupdf tqdm -q\n",
    "\n",
    "# Uncomment below if you have scanned PDFs that need OCR\n",
    "!pip install chandra-ocr -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input: d:\\github_repos\\agri_llm\\pdfs\n",
      "üìÇ Output: d:\\github_repos\\agri_llm\\extracted_data\n",
      "üìÑ Dataset: agricultural_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "PDF_DIR = Path(\"pdfs\")  # Input directory with PDFs\n",
    "OUTPUT_DIR = Path(\"extracted_data\")  # Output directory\n",
    "DATASET_FILE = OUTPUT_DIR / \"agricultural_dataset.jsonl\"  # Final dataset\n",
    "\n",
    "# Analysis settings\n",
    "SAMPLE_PAGES = 5  # Pages to check for digital text\n",
    "MIN_CHARS_PER_PAGE = 100  # Threshold for digital vs scanned\n",
    "# =======================================================\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Input: {PDF_DIR.absolute()}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"üìÑ Dataset: {DATASET_FILE.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Analyze PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 PDF files\n",
      "\n",
      "Analyzing PDFs...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd1027a9e3b4edeac48ddc9de3ce500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Analysis Results:\n",
      "================================================================================\n",
      "chapter02.pdf                                      |  14 pages |   1.6 MB | üü¢ DIGITAL\n",
      "chapter03.pdf                                      |  10 pages |   0.8 MB | üü¢ DIGITAL\n",
      "chapter06.pdf                                      |  17 pages |   1.9 MB | üü¢ DIGITAL\n",
      "chapter08.pdf                                      |  22 pages |   0.5 MB | üü¢ DIGITAL\n",
      "chapter10.pdf                                      |  10 pages |   0.2 MB | üü¢ DIGITAL\n",
      "iah_-_cropping_systems_and_alternative_crops.pdf   |  32 pages |   5.7 MB | üü¢ DIGITAL\n",
      "iah_-_nitrogen_management_for_corn_v4.pdf          |  30 pages |   1.8 MB | üü¢ DIGITAL\n",
      "iah_-_weather_climate_and_agriculture.pdf          |  14 pages |   1.4 MB | üü¢ DIGITAL\n",
      "managing_diseases_2022web.pdf                      |  20 pages |   1.0 MB | üü¢ DIGITAL\n",
      "managing_insect_pests_2022web.pdf                  |  20 pages |   2.1 MB | üü¢ DIGITAL\n",
      "nematodes_2022web.pdf                              |  14 pages |   2.5 MB | üü¢ DIGITAL\n",
      "water_management_2022web.pdf                       |  12 pages |   1.1 MB | üü¢ DIGITAL\n",
      "water_quality_2022web.pdf                          |   8 pages |   0.8 MB | üü¢ DIGITAL\n",
      "weed_management_2022web.pdf                        |  26 pages |   1.0 MB | üü¢ DIGITAL\n",
      "\n",
      "================================================================================\n",
      "üìà Summary: 14 digital, 0 scanned (total: 14)\n"
     ]
    }
   ],
   "source": [
    "def analyze_pdf(pdf_path, sample_pages=SAMPLE_PAGES):\n",
    "    \"\"\"\n",
    "    Analyze PDF to determine if it's digital (has extractable text) or scanned (needs OCR).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        sample_pages: Number of pages to check\n",
    "    \n",
    "    Returns:\n",
    "        dict with analysis results\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_chars = 0\n",
    "    pages_to_check = min(sample_pages, len(doc))\n",
    "    \n",
    "    # Check first N pages for text content\n",
    "    for i in range(pages_to_check):\n",
    "        text = doc[i].get_text().strip()\n",
    "        total_chars += len(text)\n",
    "    \n",
    "    avg_chars = total_chars / pages_to_check if pages_to_check > 0 else 0\n",
    "    is_digital = avg_chars >= MIN_CHARS_PER_PAGE\n",
    "    \n",
    "    result = {\n",
    "        \"path\": pdf_path,\n",
    "        \"name\": pdf_path.name,\n",
    "        \"stem\": pdf_path.stem,\n",
    "        \"size_mb\": pdf_path.stat().st_size / (1024 * 1024),\n",
    "        \"total_pages\": len(doc),\n",
    "        \"avg_chars_per_page\": round(avg_chars, 1),\n",
    "        \"is_digital\": is_digital,\n",
    "        \"extraction_method\": \"digital\" if is_digital else \"ocr\"\n",
    "    }\n",
    "    \n",
    "    doc.close()\n",
    "    return result\n",
    "\n",
    "# Find and analyze all PDFs\n",
    "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"‚ùå No PDF files found! Please check the PDF_DIR path.\")\n",
    "else:\n",
    "    print(\"Analyzing PDFs...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pdf_analysis = []\n",
    "    digital_count = 0\n",
    "    scanned_count = 0\n",
    "    \n",
    "    for pdf in tqdm(pdf_files, desc=\"Analyzing\"):\n",
    "        info = analyze_pdf(pdf)\n",
    "        pdf_analysis.append(info)\n",
    "        \n",
    "        if info[\"is_digital\"]:\n",
    "            digital_count += 1\n",
    "        else:\n",
    "            scanned_count += 1\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Analysis Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    for info in pdf_analysis:\n",
    "        status = \"üü¢ DIGITAL\" if info[\"is_digital\"] else \"üî¥ SCANNED (needs OCR)\"\n",
    "        print(f\"{info['name']:<50} | {info['total_pages']:3d} pages | {info['size_mb']:5.1f} MB | {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìà Summary: {digital_count} digital, {scanned_count} scanned (total: {len(pdf_files)})\")\n",
    "    \n",
    "    if scanned_count > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Some PDFs need OCR. This will be handled in Step 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 4: Extract Digital PDFs (Fast Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 14 digital PDFs...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd9116cee56417ba1a62393d612609b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting digital PDFs:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì chapter02.pdf: 14 pages extracted\n",
      "  ‚úì chapter03.pdf: 10 pages extracted\n",
      "  ‚úì chapter06.pdf: 17 pages extracted\n",
      "  ‚úì chapter08.pdf: 22 pages extracted\n",
      "  ‚úì chapter10.pdf: 10 pages extracted\n",
      "  ‚úì iah_-_cropping_systems_and_alternative_crops.pdf: 32 pages extracted\n",
      "  ‚úì iah_-_nitrogen_management_for_corn_v4.pdf: 30 pages extracted\n",
      "  ‚úì iah_-_weather_climate_and_agriculture.pdf: 14 pages extracted\n",
      "  ‚úì managing_diseases_2022web.pdf: 20 pages extracted\n",
      "  ‚úì managing_insect_pests_2022web.pdf: 20 pages extracted\n",
      "  ‚úì nematodes_2022web.pdf: 14 pages extracted\n",
      "  ‚úì water_management_2022web.pdf: 12 pages extracted\n",
      "  ‚úì water_quality_2022web.pdf: 8 pages extracted\n",
      "  ‚úì weed_management_2022web.pdf: 26 pages extracted\n",
      "\n",
      "‚úÖ Extracted 249 pages from digital PDFs\n"
     ]
    }
   ],
   "source": [
    "def extract_digital_pdf(pdf_info):\n",
    "    \"\"\"\n",
    "    Extract text from digital PDFs using PyMuPDF (fast, no OCR needed).\n",
    "    \n",
    "    Args:\n",
    "        pdf_info: Dictionary with PDF information from analyze_pdf()\n",
    "    \n",
    "    Returns:\n",
    "        List of records (one per page)\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_info['path'])\n",
    "    records = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        \n",
    "        # Only include pages with actual content\n",
    "        if text:\n",
    "            record = {\n",
    "                \"id\": f\"{pdf_info['stem']}-p{page_num + 1}\",\n",
    "                \"doc\": pdf_info['stem'],\n",
    "                \"page\": page_num + 1,\n",
    "                \"total_pages\": pdf_info['total_pages'],\n",
    "                \"text\": text,\n",
    "                \"source\": pdf_info['name'],\n",
    "                \"method\": \"digital\",\n",
    "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    doc.close()\n",
    "    return records\n",
    "\n",
    "# Extract all digital PDFs\n",
    "digital_pdfs = [info for info in pdf_analysis if info[\"is_digital\"]]\n",
    "all_records = []\n",
    "\n",
    "if digital_pdfs:\n",
    "    print(f\"Extracting {len(digital_pdfs)} digital PDFs...\\n\")\n",
    "    \n",
    "    for pdf_info in tqdm(digital_pdfs, desc=\"Extracting digital PDFs\"):\n",
    "        records = extract_digital_pdf(pdf_info)\n",
    "        all_records.extend(records)\n",
    "        print(f\"  ‚úì {pdf_info['name']}: {len(records)} pages extracted\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {len(all_records)} pages from digital PDFs\")\n",
    "else:\n",
    "    print(\"No digital PDFs found. All PDFs require OCR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: Extract Scanned PDFs (OCR Method)\n",
    "\n",
    "**Note**: This step uses Chandra OCR for scanned PDFs. If you don't have scanned PDFs, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ No scanned PDFs found. All PDFs were digital.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def extract_scanned_pdf_with_chandra(pdf_info):\n",
    "    \"\"\"\n",
    "    Extract text from scanned PDFs using Chandra OCR.\n",
    "    \n",
    "    Args:\n",
    "        pdf_info: Dictionary with PDF information\n",
    "    \n",
    "    Returns:\n",
    "        List of records (one per page)\n",
    "    \"\"\"\n",
    "    # Run Chandra OCR\n",
    "    cmd = [\n",
    "        \"chandra\",\n",
    "        str(pdf_info['path']),\n",
    "        str(OUTPUT_DIR),\n",
    "        \"--method\", \"hf\",  # Use HuggingFace method (local, no server needed)\n",
    "        \"--no-images\"  # Skip images to save space\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Running OCR on {pdf_info['name']}...\")\n",
    "    subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    \n",
    "    # Find the generated markdown file\n",
    "    md_file = OUTPUT_DIR / f\"{pdf_info['stem']}.md\"\n",
    "    \n",
    "    if not md_file.exists():\n",
    "        print(f\"  ‚ö†Ô∏è  Warning: Expected output file {md_file} not found\")\n",
    "        return []\n",
    "    \n",
    "    # Read and parse the markdown output\n",
    "    with open(md_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by page markers (Chandra typically uses ---)\n",
    "    if '\\n---\\n' in content:\n",
    "        pages = content.split('\\n---\\n')\n",
    "    else:\n",
    "        pages = [content]\n",
    "    \n",
    "    # Create records\n",
    "    records = []\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        page_text = page_text.strip()\n",
    "        if page_text:\n",
    "            record = {\n",
    "                \"id\": f\"{pdf_info['stem']}-p{page_num}\",\n",
    "                \"doc\": pdf_info['stem'],\n",
    "                \"page\": page_num,\n",
    "                \"total_pages\": len(pages),\n",
    "                \"text\": page_text,\n",
    "                \"source\": pdf_info['name'],\n",
    "                \"method\": \"ocr\",\n",
    "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Extract scanned PDFs\n",
    "scanned_pdfs = [info for info in pdf_analysis if not info[\"is_digital\"]]\n",
    "\n",
    "if scanned_pdfs:\n",
    "    print(f\"\\nProcessing {len(scanned_pdfs)} scanned PDFs with Chandra OCR...\")\n",
    "    print(\"‚ö†Ô∏è  This may take several minutes per document.\\n\")\n",
    "    \n",
    "    for pdf_info in scanned_pdfs:\n",
    "        try:\n",
    "            records = extract_scanned_pdf_with_chandra(pdf_info)\n",
    "            all_records.extend(records)\n",
    "            print(f\"  ‚úì {pdf_info['name']}: {len(records)} pages extracted\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {pdf_info['name']}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OCR extraction complete\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No scanned PDFs found. All PDFs were digital.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Save Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATASET STATISTICS\n",
      "================================================================================\n",
      "Total documents:        14\n",
      "Total pages:            249\n",
      "Total characters:       967,037\n",
      "Avg chars per page:     3,884\n",
      "\n",
      "Dataset saved to:       extracted_data\\agricultural_dataset.jsonl\n",
      "File size:              1.00 MB\n",
      "================================================================================\n",
      "\n",
      "üìà Extraction Methods:\n",
      "  Digital (fast):       249 pages\n",
      "  OCR (Chandra):        0 pages\n"
     ]
    }
   ],
   "source": [
    "# Save all records to JSONL\n",
    "with open(DATASET_FILE, 'w', encoding='utf-8') as f:\n",
    "    for record in all_records:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Calculate statistics\n",
    "total_pages = len(all_records)\n",
    "total_docs = len(set(r['doc'] for r in all_records))\n",
    "total_chars = sum(len(r['text']) for r in all_records)\n",
    "avg_chars_per_page = total_chars / total_pages if total_pages > 0 else 0\n",
    "\n",
    "print(\"=\"* 80)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\"* 80)\n",
    "print(f\"Total documents:        {total_docs}\")\n",
    "print(f\"Total pages:            {total_pages:,}\")\n",
    "print(f\"Total characters:       {total_chars:,}\")\n",
    "print(f\"Avg chars per page:     {avg_chars_per_page:,.0f}\")\n",
    "print(f\"\\nDataset saved to:       {DATASET_FILE}\")\n",
    "print(f\"File size:              {DATASET_FILE.stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(\"=\"* 80)\n",
    "\n",
    "# Show extraction method breakdown\n",
    "digital_count = sum(1 for r in all_records if r['method'] == 'digital')\n",
    "ocr_count = sum(1 for r in all_records if r['method'] == 'ocr')\n",
    "print(f\"\\nüìà Extraction Methods:\")\n",
    "print(f\"  Digital (fast):       {digital_count} pages\")\n",
    "print(f\"  OCR (Chandra):        {ocr_count} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Step 7: Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ DATASET PREVIEW (First 3 records)\n",
      "\n",
      "================================================================================\n",
      "Record 1/3\n",
      "================================================================================\n",
      "ID:              chapter02-p1\n",
      "Document:        chapter02\n",
      "Page:            1/14\n",
      "Source:          chapter02.pdf\n",
      "Method:          digital\n",
      "Text length:     3366 characters\n",
      "\n",
      "Text preview (first 500 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Corn\t \t\n",
      "\t\n",
      "   \t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      " \n",
      " \n",
      "13\n",
      "2 \n",
      "Corn\n",
      "Emerson Nafziger\n",
      "Department of Crop Sciences\n",
      "ednaf@illinois.edu\n",
      "C\n",
      "orn was an important crop for people who lived in \n",
      "the area that became Illinois before the Europeans \n",
      "first set foot here; it was the staple food crop of the people \n",
      "who lived in the Cahokia area some 1,000 years ago. It \n",
      "was a crop of choice when Europeans settled and started \n",
      "to farm in Illinois, and acreage in the state first reached 10 \n",
      "million acres in 1895. Acreage over the past 100 yea...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Record 2/3\n",
      "================================================================================\n",
      "ID:              chapter02-p2\n",
      "Document:        chapter02\n",
      "Page:            2/14\n",
      "Source:          chapter02.pdf\n",
      "Method:          digital\n",
      "Text length:     2393 characters\n",
      "\n",
      "Text preview (first 500 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "14\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "          Illinois Agronomy Handbook\n",
      "l Rn, or ‚Äúreproductive‚Äù stage n, where n goes from 1 \n",
      "(silking, which coincides with pollen shed) to R6, which is \n",
      "physiological maturity.\n",
      "This staging system is almost universally used, though \n",
      "other methods in use count leaves when they have most of \n",
      "their area exposed, which occurs several days before the \n",
      "collar appears. \n",
      "Many years ago scientists observed that corn plant devel-\n",
      "opment follows very closely the accumulation of average \n",
      "...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Record 3/3\n",
      "================================================================================\n",
      "ID:              chapter02-p3\n",
      "Document:        chapter02\n",
      "Page:            3/14\n",
      "Source:          chapter02.pdf\n",
      "Method:          digital\n",
      "Text length:     3480 characters\n",
      "\n",
      "Text preview (first 500 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "Corn\t \t\n",
      "\t\n",
      "   \t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      " \n",
      " \n",
      "15\n",
      "Corn hybrids grown in Illinois have planting-to-harvest \n",
      "GDD requirements ranging from 2,200 to 2,400 for early \n",
      "hybrids grown in the northern part of the state to 2,800 \n",
      "to 2,900 for late hybrids grown in the southernmost part \n",
      "of the state. A full-season hybrid for a particular area \n",
      "generally matures in several hundred fewer GDD than the \n",
      "number given in Figure 2.4. Thus, a full-season hybrid for \n",
      "northern Illinois would be one that matures in about 2,600 \n",
      "GDD, w...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preview first 3 records\n",
    "with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "    samples = [json.loads(line) for i, line in enumerate(f) if i < 3]\n",
    "\n",
    "print(\"üìñ DATASET PREVIEW (First 3 records)\\n\")\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Record {i}/{len(samples)}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ID:              {sample['id']}\")\n",
    "    print(f\"Document:        {sample['doc']}\")\n",
    "    print(f\"Page:            {sample['page']}/{sample['total_pages']}\")\n",
    "    print(f\"Source:          {sample['source']}\")\n",
    "    print(f\"Method:          {sample['method']}\")\n",
    "    print(f\"Text length:     {len(sample['text'])} characters\")\n",
    "    print(f\"\\nText preview (first 500 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    preview = sample['text'][:500]\n",
    "    print(preview + (\"...\" if len(sample['text']) > 500 else \"\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 8: Document-Level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö DOCUMENT-LEVEL STATISTICS\n",
      "==========================================================================================\n",
      "Document                                      |  Pages |      Chars |   Method\n",
      "==========================================================================================\n",
      "chapter02.pdf                                 |     14 |     64,393 |  digital\n",
      "chapter03.pdf                                 |     10 |     48,932 |  digital\n",
      "chapter06.pdf                                 |     17 |     73,364 |  digital\n",
      "chapter08.pdf                                 |     22 |    100,167 |  digital\n",
      "chapter10.pdf                                 |     10 |     48,348 |  digital\n",
      "iah_-_cropping_systems_and_alternative_crops.pdf |     32 |    129,404 |  digital\n",
      "iah_-_nitrogen_management_for_corn_v4.pdf     |     30 |    128,497 |  digital\n",
      "iah_-_weather_climate_and_agriculture.pdf     |     14 |     42,089 |  digital\n",
      "managing_diseases_2022web.pdf                 |     20 |     62,162 |  digital\n",
      "managing_insect_pests_2022web.pdf             |     20 |     72,070 |  digital\n",
      "nematodes_2022web.pdf                         |     14 |     37,703 |  digital\n",
      "water_management_2022web.pdf                  |     12 |     39,692 |  digital\n",
      "water_quality_2022web.pdf                     |      8 |     28,320 |  digital\n",
      "weed_management_2022web.pdf                   |     26 |     91,896 |  digital\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze each document\n",
    "from collections import defaultdict\n",
    "\n",
    "doc_stats = defaultdict(lambda: {'pages': 0, 'chars': 0})\n",
    "\n",
    "for record in all_records:\n",
    "    doc_stats[record['doc']]['pages'] += 1\n",
    "    doc_stats[record['doc']]['chars'] += len(record['text'])\n",
    "    doc_stats[record['doc']]['source'] = record['source']\n",
    "    doc_stats[record['doc']]['method'] = record['method']\n",
    "\n",
    "print(\"üìö DOCUMENT-LEVEL STATISTICS\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Document':<45} | {'Pages':>6} | {'Chars':>10} | {'Method':>8}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for doc_name, stats in sorted(doc_stats.items()):\n",
    "    print(f\"{stats['source']:<45} | {stats['pages']:>6} | {stats['chars']:>10,} | {stats['method']:>8}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "Your dataset is ready! Here's what you can do next:\n",
    "\n",
    "### 1. Load the Dataset\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Load all records\n",
    "with open('extracted_data/agricultural_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Or load with HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='extracted_data/agricultural_dataset.jsonl')\n",
    "```\n",
    "\n",
    "### 2. Generate Q&A Pairs (Next Phase)\n",
    "- Use GPT-4 or Claude to generate question-answer pairs\n",
    "- Use the text as context for creating training examples\n",
    "- Format for instruction fine-tuning\n",
    "\n",
    "### 3. Create Embeddings for RAG\n",
    "- Use OpenAI embeddings or open-source alternatives\n",
    "- Store in vector database (Pinecone, Chroma, FAISS)\n",
    "- Build retrieval-augmented generation system\n",
    "\n",
    "### 4. Fine-tune an LLM\n",
    "- Use for continued pre-training on agricultural domain\n",
    "- Or create instruction dataset first, then fine-tune\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Location**: `extracted_data/agricultural_dataset.jsonl`  \n",
    "**Format**: One JSON object per line (JSONL)  \n",
    "**Ready for**: LLM training, Q&A generation, embeddings, RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agri_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
