{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Agricultural PDF Text Extraction Pipeline\n",
    "\n",
    "This notebook extracts text from agricultural PDFs and creates a unified dataset for LLM fine-tuning.\n",
    "\n",
    "## Features:\n",
    "- **Smart Detection**: Automatically detects digital vs scanned PDFs\n",
    "- **Fast Extraction**: Uses PyMuPDF for digital PDFs (instant)\n",
    "- **High-Quality OCR**: Uses Chandra for scanned PDFs\n",
    "- **JSONL Output**: Best format for LLM training\n",
    "- **Rich Metadata**: Includes source, page numbers, timestamps\n",
    "\n",
    "## Output Format:\n",
    "```json\n",
    "{\"id\": \"chapter02-p1\", \"doc\": \"chapter02\", \"page\": 1, \"text\": \"...\", \"source\": \"chapter02.pdf\", \"method\": \"digital\", \"timestamp\": \"...\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pymupdf tqdm -q\n",
    "\n",
    "# Uncomment below if you have scanned PDFs that need OCR\n",
    "# !pip install chandra-ocr -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "PDF_DIR = Path(\"pdfs\")  # Input directory with PDFs\n",
    "OUTPUT_DIR = Path(\"extracted_data\")  # Output directory\n",
    "DATASET_FILE = OUTPUT_DIR / \"agricultural_dataset.jsonl\"  # Final dataset\n",
    "\n",
    "# Analysis settings\n",
    "SAMPLE_PAGES = 5  # Pages to check for digital text\n",
    "MIN_CHARS_PER_PAGE = 100  # Threshold for digital vs scanned\n",
    "# =======================================================\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Input: {PDF_DIR.absolute()}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"üìÑ Dataset: {DATASET_FILE.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Analyze PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pdf(pdf_path, sample_pages=SAMPLE_PAGES):\n",
    "    \"\"\"\n",
    "    Analyze PDF to determine if it's digital (has extractable text) or scanned (needs OCR).\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        sample_pages: Number of pages to check\n",
    "    \n",
    "    Returns:\n",
    "        dict with analysis results\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_chars = 0\n",
    "    pages_to_check = min(sample_pages, len(doc))\n",
    "    \n",
    "    # Check first N pages for text content\n",
    "    for i in range(pages_to_check):\n",
    "        text = doc[i].get_text().strip()\n",
    "        total_chars += len(text)\n",
    "    \n",
    "    avg_chars = total_chars / pages_to_check if pages_to_check > 0 else 0\n",
    "    is_digital = avg_chars >= MIN_CHARS_PER_PAGE\n",
    "    \n",
    "    result = {\n",
    "        \"path\": pdf_path,\n",
    "        \"name\": pdf_path.name,\n",
    "        \"stem\": pdf_path.stem,\n",
    "        \"size_mb\": pdf_path.stat().st_size / (1024 * 1024),\n",
    "        \"total_pages\": len(doc),\n",
    "        \"avg_chars_per_page\": round(avg_chars, 1),\n",
    "        \"is_digital\": is_digital,\n",
    "        \"extraction_method\": \"digital\" if is_digital else \"ocr\"\n",
    "    }\n",
    "    \n",
    "    doc.close()\n",
    "    return result\n",
    "\n",
    "# Find and analyze all PDFs\n",
    "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "\n",
    "if not pdf_files:\n",
    "    print(\"‚ùå No PDF files found! Please check the PDF_DIR path.\")\n",
    "else:\n",
    "    print(\"Analyzing PDFs...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pdf_analysis = []\n",
    "    digital_count = 0\n",
    "    scanned_count = 0\n",
    "    \n",
    "    for pdf in tqdm(pdf_files, desc=\"Analyzing\"):\n",
    "        info = analyze_pdf(pdf)\n",
    "        pdf_analysis.append(info)\n",
    "        \n",
    "        if info[\"is_digital\"]:\n",
    "            digital_count += 1\n",
    "        else:\n",
    "            scanned_count += 1\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìä Analysis Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    for info in pdf_analysis:\n",
    "        status = \"üü¢ DIGITAL\" if info[\"is_digital\"] else \"üî¥ SCANNED (needs OCR)\"\n",
    "        print(f\"{info['name']:<50} | {info['total_pages']:3d} pages | {info['size_mb']:5.1f} MB | {status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìà Summary: {digital_count} digital, {scanned_count} scanned (total: {len(pdf_files)})\")\n",
    "    \n",
    "    if scanned_count > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Some PDFs need OCR. This will be handled in Step 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Step 4: Extract Digital PDFs (Fast Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digital_pdf(pdf_info):\n",
    "    \"\"\"\n",
    "    Extract text from digital PDFs using PyMuPDF (fast, no OCR needed).\n",
    "    \n",
    "    Args:\n",
    "        pdf_info: Dictionary with PDF information from analyze_pdf()\n",
    "    \n",
    "    Returns:\n",
    "        List of records (one per page)\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_info['path'])\n",
    "    records = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        \n",
    "        # Only include pages with actual content\n",
    "        if text:\n",
    "            record = {\n",
    "                \"id\": f\"{pdf_info['stem']}-p{page_num + 1}\",\n",
    "                \"doc\": pdf_info['stem'],\n",
    "                \"page\": page_num + 1,\n",
    "                \"total_pages\": pdf_info['total_pages'],\n",
    "                \"text\": text,\n",
    "                \"source\": pdf_info['name'],\n",
    "                \"method\": \"digital\",\n",
    "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    doc.close()\n",
    "    return records\n",
    "\n",
    "# Extract all digital PDFs\n",
    "digital_pdfs = [info for info in pdf_analysis if info[\"is_digital\"]]\n",
    "all_records = []\n",
    "\n",
    "if digital_pdfs:\n",
    "    print(f\"Extracting {len(digital_pdfs)} digital PDFs...\\n\")\n",
    "    \n",
    "    for pdf_info in tqdm(digital_pdfs, desc=\"Extracting digital PDFs\"):\n",
    "        records = extract_digital_pdf(pdf_info)\n",
    "        all_records.extend(records)\n",
    "        print(f\"  ‚úì {pdf_info['name']}: {len(records)} pages extracted\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {len(all_records)} pages from digital PDFs\")\n",
    "else:\n",
    "    print(\"No digital PDFs found. All PDFs require OCR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: Extract Scanned PDFs (OCR Method)\n",
    "\n",
    "**Note**: This step uses Chandra OCR for scanned PDFs. If you don't have scanned PDFs, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def extract_scanned_pdf_with_chandra(pdf_info):\n",
    "    \"\"\"\n",
    "    Extract text from scanned PDFs using Chandra OCR.\n",
    "    \n",
    "    Args:\n",
    "        pdf_info: Dictionary with PDF information\n",
    "    \n",
    "    Returns:\n",
    "        List of records (one per page)\n",
    "    \"\"\"\n",
    "    # Run Chandra OCR\n",
    "    cmd = [\n",
    "        \"chandra\",\n",
    "        str(pdf_info['path']),\n",
    "        str(OUTPUT_DIR),\n",
    "        \"--method\", \"hf\",  # Use HuggingFace method (local, no server needed)\n",
    "        \"--no-images\"  # Skip images to save space\n",
    "    ]\n",
    "    \n",
    "    print(f\"  Running OCR on {pdf_info['name']}...\")\n",
    "    subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "    \n",
    "    # Find the generated markdown file\n",
    "    md_file = OUTPUT_DIR / f\"{pdf_info['stem']}.md\"\n",
    "    \n",
    "    if not md_file.exists():\n",
    "        print(f\"  ‚ö†Ô∏è  Warning: Expected output file {md_file} not found\")\n",
    "        return []\n",
    "    \n",
    "    # Read and parse the markdown output\n",
    "    with open(md_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by page markers (Chandra typically uses ---)\n",
    "    if '\\n---\\n' in content:\n",
    "        pages = content.split('\\n---\\n')\n",
    "    else:\n",
    "        pages = [content]\n",
    "    \n",
    "    # Create records\n",
    "    records = []\n",
    "    for page_num, page_text in enumerate(pages, start=1):\n",
    "        page_text = page_text.strip()\n",
    "        if page_text:\n",
    "            record = {\n",
    "                \"id\": f\"{pdf_info['stem']}-p{page_num}\",\n",
    "                \"doc\": pdf_info['stem'],\n",
    "                \"page\": page_num,\n",
    "                \"total_pages\": len(pages),\n",
    "                \"text\": page_text,\n",
    "                \"source\": pdf_info['name'],\n",
    "                \"method\": \"ocr\",\n",
    "                \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Extract scanned PDFs\n",
    "scanned_pdfs = [info for info in pdf_analysis if not info[\"is_digital\"]]\n",
    "\n",
    "if scanned_pdfs:\n",
    "    print(f\"\\nProcessing {len(scanned_pdfs)} scanned PDFs with Chandra OCR...\")\n",
    "    print(\"‚ö†Ô∏è  This may take several minutes per document.\\n\")\n",
    "    \n",
    "    for pdf_info in scanned_pdfs:\n",
    "        try:\n",
    "            records = extract_scanned_pdf_with_chandra(pdf_info)\n",
    "            all_records.extend(records)\n",
    "            print(f\"  ‚úì {pdf_info['name']}: {len(records)} pages extracted\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {pdf_info['name']}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ OCR extraction complete\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No scanned PDFs found. All PDFs were digital.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Save Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all records to JSONL\n",
    "with open(DATASET_FILE, 'w', encoding='utf-8') as f:\n",
    "    for record in all_records:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Calculate statistics\n",
    "total_pages = len(all_records)\n",
    "total_docs = len(set(r['doc'] for r in all_records))\n",
    "total_chars = sum(len(r['text']) for r in all_records)\n",
    "avg_chars_per_page = total_chars / total_pages if total_pages > 0 else 0\n",
    "\n",
    "print(\"=\"* 80)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\"* 80)\n",
    "print(f\"Total documents:        {total_docs}\")\n",
    "print(f\"Total pages:            {total_pages:,}\")\n",
    "print(f\"Total characters:       {total_chars:,}\")\n",
    "print(f\"Avg chars per page:     {avg_chars_per_page:,.0f}\")\n",
    "print(f\"\\nDataset saved to:       {DATASET_FILE}\")\n",
    "print(f\"File size:              {DATASET_FILE.stat().st_size / (1024*1024):.2f} MB\")\n",
    "print(\"=\"* 80)\n",
    "\n",
    "# Show extraction method breakdown\n",
    "digital_count = sum(1 for r in all_records if r['method'] == 'digital')\n",
    "ocr_count = sum(1 for r in all_records if r['method'] == 'ocr')\n",
    "print(f\"\\nüìà Extraction Methods:\")\n",
    "print(f\"  Digital (fast):       {digital_count} pages\")\n",
    "print(f\"  OCR (Chandra):        {ocr_count} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëÄ Step 7: Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preview first 3 records\n",
    "with open(DATASET_FILE, 'r', encoding='utf-8') as f:\n",
    "    samples = [json.loads(line) for i, line in enumerate(f) if i < 3]\n",
    "\n",
    "print(\"üìñ DATASET PREVIEW (First 3 records)\\n\")\n",
    "\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Record {i}/{len(samples)}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ID:              {sample['id']}\")\n",
    "    print(f\"Document:        {sample['doc']}\")\n",
    "    print(f\"Page:            {sample['page']}/{sample['total_pages']}\")\n",
    "    print(f\"Source:          {sample['source']}\")\n",
    "    print(f\"Method:          {sample['method']}\")\n",
    "    print(f\"Text length:     {len(sample['text'])} characters\")\n",
    "    print(f\"\\nText preview (first 500 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    preview = sample['text'][:500]\n",
    "    print(preview + (\"...\" if len(sample['text']) > 500 else \"\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 8: Document-Level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each document\n",
    "from collections import defaultdict\n",
    "\n",
    "doc_stats = defaultdict(lambda: {'pages': 0, 'chars': 0})\n",
    "\n",
    "for record in all_records:\n",
    "    doc_stats[record['doc']]['pages'] += 1\n",
    "    doc_stats[record['doc']]['chars'] += len(record['text'])\n",
    "    doc_stats[record['doc']]['source'] = record['source']\n",
    "    doc_stats[record['doc']]['method'] = record['method']\n",
    "\n",
    "print(\"üìö DOCUMENT-LEVEL STATISTICS\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Document':<45} | {'Pages':>6} | {'Chars':>10} | {'Method':>8}\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for doc_name, stats in sorted(doc_stats.items()):\n",
    "    print(f\"{stats['source']:<45} | {stats['pages']:>6} | {stats['chars']:>10,} | {stats['method']:>8}\")\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Next Steps\n",
    "\n",
    "Your dataset is ready! Here's what you can do next:\n",
    "\n",
    "### 1. Load the Dataset\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Load all records\n",
    "with open('extracted_data/agricultural_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Or load with HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='extracted_data/agricultural_dataset.jsonl')\n",
    "```\n",
    "\n",
    "### 2. Generate Q&A Pairs (Next Phase)\n",
    "- Use GPT-4 or Claude to generate question-answer pairs\n",
    "- Use the text as context for creating training examples\n",
    "- Format for instruction fine-tuning\n",
    "\n",
    "### 3. Create Embeddings for RAG\n",
    "- Use OpenAI embeddings or open-source alternatives\n",
    "- Store in vector database (Pinecone, Chroma, FAISS)\n",
    "- Build retrieval-augmented generation system\n",
    "\n",
    "### 4. Fine-tune an LLM\n",
    "- Use for continued pre-training on agricultural domain\n",
    "- Or create instruction dataset first, then fine-tune\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Location**: `extracted_data/agricultural_dataset.jsonl`  \n",
    "**Format**: One JSON object per line (JSONL)  \n",
    "**Ready for**: LLM training, Q&A generation, embeddings, RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
